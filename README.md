# output-template extension for text-generation-webui

Proof of concept. Aim of this code is to force output generated by LLM to conform with expected format,
making it possible to parse it.

This is done by creating state machine which, every time when new token is to be generated, decides which
tokens are allowed to conform with required format and ban (set probability to -inf) all other options.

To better explain the issue, here is [an example of prompt and output generated without extension demonstrating the problem](https://rentry.org/yxg7s)
and here is [same prompt and output generated with template](https://rentry.org/9nzyr)
